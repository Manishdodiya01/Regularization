{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a3626d-eaaa-44aa-ba72-b0610d351674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PART - 1 : UNDERSTANDING REGULARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78682c-f864-4092-8e53-1be6edf764b6",
   "metadata": {},
   "source": [
    "## ^k What is regularization in the context of deep learningH Why is it importantG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dcf2c-6b9b-4eb5-8049-1246ceb2796e",
   "metadata": {},
   "source": [
    "Regularization in the context of deep learning refers to a set of techniques used to prevent a model from overfitting the training data. Overfitting occurs when a model learns not only the underlying patterns in the training data but also captures noise and random fluctuations that are present in that specific dataset. As a result, the model performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "Regularization methods aim to address overfitting by adding a penalty term to the loss function. This penalty discourages the model from learning overly complex patterns that might not generalize well. Two common types of regularization in deep learning are L1 regularization and L2 regularization:\n",
    "\n",
    "1. **L1 Regularization (Lasso):** It adds the absolute values of the coefficients to the loss function. This encourages sparsity in the weight matrix, leading some weights to become exactly zero. Sparse models are more interpretable and may perform better on unseen data.\n",
    "\n",
    "2. **L2 Regularization (Ridge):** It adds the squared values of the coefficients to the loss function. This penalizes large weights, preventing the model from relying too heavily on a small number of features. L2 regularization helps smooth out the learned parameters.\n",
    "\n",
    "In addition to L1 and L2 regularization, there are other techniques such as dropout, which randomly drops out a fraction of neurons during training, and early stopping, which stops training once the model's performance on a validation set starts to degrade.\n",
    "\n",
    "The importance of regularization in deep learning lies in its ability to improve a model's generalization performance. Without regularization, a neural network might fit the training data too closely, capturing noise and making it less effective on new, unseen data. Regularization helps strike a balance between fitting the training data well and avoiding overfitting, ultimately leading to better performance on new data. It is an essential tool for building more robust and reliable deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6c8da-745f-4049-ad2e-25d8b64d099b",
   "metadata": {},
   "source": [
    "## Ek Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoffk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e519c5e-a10c-4816-8bc0-a8fb657c919c",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning, including deep learning, that describes the balance between two sources of error in a predictive model: bias and variance.\n",
    "\n",
    "1. **Bias:** Bias refers to the error introduced by approximating a real-world problem with a simplified model. A high-bias model makes strong assumptions about the data and may oversimplify the underlying patterns, leading to systematic errors. It tends to underfit the training data.\n",
    "\n",
    "2. **Variance:** Variance, on the other hand, measures the model's sensitivity to fluctuations in the training data. A high-variance model is flexible and can capture intricate patterns, but it is more likely to fit noise and random fluctuations in the training data, leading to poor generalization on new, unseen data. It tends to overfit the training data.\n",
    "\n",
    "The bias-variance tradeoff implies that as you decrease bias, variance increases, and vice versa. Achieving a good balance is crucial for building a model that generalizes well to new data. Regularization plays a key role in addressing the bias-variance tradeoff by controlling the complexity of a model.\n",
    "\n",
    "Here's how regularization helps in this context:\n",
    "\n",
    "1. **Bias Reduction:** Regularization methods, such as L1 and L2 regularization, introduce penalty terms that discourage the model from assigning too much importance to any single feature or from having large weights. This helps in reducing the model's bias by preventing it from oversimplifying the underlying patterns in the data.\n",
    "\n",
    "2. **Variance Reduction:** By penalizing large weights or encouraging sparsity in the model, regularization methods reduce the model's capacity to fit noise in the training data. This helps in reducing variance, making the model less sensitive to small fluctuations and improving its ability to generalize to new data.\n",
    "\n",
    "In summary, regularization helps in finding an optimal point in the bias-variance tradeoff. It adds a penalty for complexity during the training process, preventing the model from becoming too flexible and fitting noise. This results in a more balanced model that generalizes well to unseen data, addressing both bias and variance to create a model that performs well across a variety of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9d723-d9be-4006-9ba9-23dc914b85bc",
   "metadata": {},
   "source": [
    "## >k Describe the concept of =1 and =2 regularization. How do they differ in terms of penalty calculation and their effects on the modelG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d0008-6176-4642-aa6d-ff69978efda9",
   "metadata": {},
   "source": [
    "It seems like there might be a formatting issue in your question. It appears that you're asking about \"=1\" and \"=2\" regularization, which may be related to L1 and L2 regularization. I'll provide an explanation based on that assumption.\n",
    "\n",
    "1. **L1 Regularization (or Lasso Regularization):**\n",
    "   - **Penalty Calculation:** L1 regularization adds the sum of the absolute values of the weights to the loss function. Mathematically, the L1 penalty term is the absolute sum of the individual weights: \\(\\lambda \\sum_{i=1}^{n} |w_i|\\), where \\(w_i\\) are the model weights and \\(\\lambda\\) is the regularization strength.\n",
    "   - **Effect on the Model:** L1 regularization encourages sparsity in the weight matrix, meaning that some weights can become exactly zero. This has the effect of feature selection, as the model may end up relying on a subset of the most important features while ignoring others. Sparse models are more interpretable and may be beneficial when dealing with high-dimensional datasets.\n",
    "\n",
    "2. **L2 Regularization (or Ridge Regularization):**\n",
    "   - **Penalty Calculation:** L2 regularization adds the sum of the squared values of the weights to the loss function. Mathematically, the L2 penalty term is the squared sum of the individual weights: \\(\\lambda \\sum_{i=1}^{n} w_i^2\\), where \\(w_i\\) are the model weights and \\(\\lambda\\) is the regularization strength.\n",
    "   - **Effect on the Model:** L2 regularization penalizes large weights but typically does not lead to exactly zero weights. It tends to spread the impact of the weights more evenly across all features. This can result in a smoother model and helps prevent any single feature from dominating the model's predictions. L2 regularization is effective when all features contribute meaningfully to the model's performance.\n",
    "\n",
    "In summary, the key differences between L1 and L2 regularization lie in their penalty calculations and their effects on the model. L1 regularization encourages sparsity by adding the absolute values of weights, leading to some weights becoming exactly zero. L2 regularization penalizes large weights by adding the squared values of weights, promoting a more balanced use of all features without driving any particular weight to zero. Practically, a combination of both L1 and L2 regularization, known as Elastic Net regularization, is sometimes used to benefit from the strengths of both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63d081-c61e-4231-bd5f-35299eb6cf21",
   "metadata": {},
   "source": [
    "## >k Describe the concept of =1 and =2 regularization. How do they differ in terms of penalty calculation and their effects on the modelG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0317fd0-0a69-427f-a085-eccae4fd8c4b",
   "metadata": {},
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model learns the training data too well, capturing noise and details that are specific to that dataset but do not generalize well to new, unseen data. Regularization techniques help address overfitting by controlling the complexity of the model and discouraging it from fitting the training data too closely. Here are the key ways in which regularization achieves this:\n",
    "\n",
    "1. **Penalizing Complexity:** Regularization methods, such as L1 and L2 regularization, add penalty terms to the loss function based on the weights of the model. These penalty terms penalize overly complex models by discouraging the use of large weights or encouraging sparsity in the weight matrix. As a result, the model is less likely to memorize the training data and is forced to focus on the most important patterns.\n",
    "\n",
    "2. **Preventing Overemphasis on Specific Features:** Regularization helps prevent the model from assigning too much importance to individual features in the training data. By penalizing large weights or encouraging sparsity, regularization ensures that the model considers a more balanced combination of features rather than relying heavily on a subset. This is particularly important when dealing with high-dimensional data where some features may be noisy or irrelevant.\n",
    "\n",
    "3. **Improving Model Robustness:** Regularization makes the model more robust by limiting its sensitivity to small variations in the training data. This helps the model generalize better to new, unseen data by avoiding the capture of noise and random fluctuations present in the training set. A more robust model is better equipped to handle diverse inputs and is less likely to make predictions based on idiosyncrasies of the training data.\n",
    "\n",
    "4. **Controlling Model Capacity:** Regularization acts as a tool for controlling the capacity of the model. Models with too much capacity (highly flexible) can fit the training data too closely and are prone to overfitting. Regularization prevents the model from becoming overly complex, striking a balance that allows it to capture essential patterns while avoiding the memorization of noise.\n",
    "\n",
    "5. **Facilitating Transfer Learning:** Regularization is especially beneficial in transfer learning scenarios where a pre-trained model is fine-tuned on a new task or dataset. It helps prevent overfitting to the small amount of new data, allowing the model to leverage the knowledge gained from the pre-training effectively.\n",
    "\n",
    "In summary, regularization is a critical component in the training of deep learning models as it helps prevent overfitting by controlling model complexity, encouraging feature selection, and improving generalization to new, unseen data. It allows models to learn meaningful patterns while avoiding the pitfalls of memorizing noise in the training data, ultimately leading to more robust and reliable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778bd4f-502e-46d9-bef0-e52b23c4744a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PART - 2 : REGULARIZATON TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff0d5a-dbdf-4d61-aa0b-2d725701cab1",
   "metadata": {},
   "source": [
    "## ¥k Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inferencek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c56840-b6e6-4c73-955f-3151d8f815a6",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique commonly used in neural networks to reduce overfitting. It was introduced by Srivastava et al. in their paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\" Dropout works by randomly \"dropping out\" a fraction of the neurons (units) in the neural network during training. This means that, during each training iteration, a random set of neurons is ignored, and their contributions to the network are temporarily removed.\n",
    "\n",
    "Here's how Dropout works and its impact on model training and inference:\n",
    "\n",
    "### Dropout during Training:\n",
    "\n",
    "1. **Randomly Dropped Neurons:** In each training iteration, a random subset of neurons is \"dropped out\" or set to zero. This means that the forward pass and backward pass of the network are performed without considering the contributions of these dropped neurons.\n",
    "\n",
    "2. **Ensemble of Subnetworks:** Dropout effectively trains an ensemble of subnetworks, each obtained by removing different sets of neurons. This helps prevent the network from relying too heavily on any specific set of features, promoting a more robust and generalized model.\n",
    "\n",
    "3. **Reduces Co-Adaptation:** Neurons in a neural network can develop co-adaptations where they rely on each other to make predictions. Dropout breaks these dependencies, forcing each neuron to be more self-reliant and reducing the risk of overfitting.\n",
    "\n",
    "4. **Approximates Model Averaging:** The use of dropout during training can be seen as a form of model averaging. It's akin to training multiple models and averaging their predictions, but it's achieved more efficiently by sharing parameters.\n",
    "\n",
    "### Impact on Model Training:\n",
    "\n",
    "1. **Regularization:** Dropout acts as a regularization technique by preventing overfitting. It helps to create a more generalized model that performs well on unseen data.\n",
    "\n",
    "2. **Reduced Sensitivity to Specific Neurons:** The network becomes less sensitive to the presence of specific neurons during training, making it more adaptable to different variations in the data.\n",
    "\n",
    "3. **Smoothing Effect:** Dropout has a smoothing effect on the optimization landscape, making it less likely for the model to get stuck in local minima. This can improve convergence during training.\n",
    "\n",
    "### Dropout during Inference:\n",
    "\n",
    "During the inference or prediction phase, dropout is typically turned off, and the full network is used. However, the weights of the neurons are usually scaled by the dropout probability to account for the fact that more neurons were active during training. This scaling ensures that the expected contribution of each neuron remains the same as during training.\n",
    "\n",
    "### Impact on Model Inference:\n",
    "\n",
    "1. **Ensemble-like Predictions:** Although dropout is turned off during inference, the model effectively approximates an ensemble of subnetworks. This can lead to more robust predictions as it combines the knowledge learned from different dropped-out configurations.\n",
    "\n",
    "2. **Reduced Sensitivity to Specific Features:** The model remains less sensitive to specific features during inference, making it more resilient to noise and variations in the input data.\n",
    "\n",
    "In summary, Dropout is a powerful regularization technique that helps prevent overfitting by randomly dropping out neurons during training. It encourages the development of a more robust and generalized model, and during inference, it retains some of the benefits of ensemble learning. However, it's important to note that while Dropout is effective for many tasks, its optimal usage may vary depending on the specific characteristics of the dataset and the neural network architecture.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce34d4-e64b-40cd-ac00-2e34c8f91b9c",
   "metadata": {},
   "source": [
    "## }k Describe the concept of Early ztopping as a form of regularization. How does it help prevent overfitting during the training processG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc0a3-cba7-493d-8267-9a367dbc47eb",
   "metadata": {},
   "source": [
    "**Early Stopping:**\n",
    "\n",
    "Early stopping is a regularization technique used in the training of machine learning models, including deep learning models, to prevent overfitting. Instead of training a model for a fixed number of epochs, early stopping monitors the model's performance on a validation set during training and stops the training process when the performance on the validation set starts to degrade.\n",
    "\n",
    "Here's how early stopping works:\n",
    "\n",
    "1. **Monitoring Validation Performance:** The model's performance is evaluated on a separate validation set after each training epoch. This can involve calculating a performance metric such as accuracy or loss on the validation set.\n",
    "\n",
    "2. **Early Stopping Criteria:** A predefined metric, such as validation loss or accuracy, is monitored. If the metric does not improve or starts to worsen over a certain number of consecutive epochs (patience), early stopping is triggered.\n",
    "\n",
    "3. **Stopping Training:** Once the early stopping criteria are met, the training process is halted, and the model with the best performance on the validation set is selected as the final model.\n",
    "\n",
    "**How Early Stopping Prevents Overfitting:**\n",
    "\n",
    "1. **Generalization Control:** Early stopping prevents the model from training for too many epochs, which can lead to overfitting. Overfitting occurs when a model becomes too tailored to the training data, capturing noise and patterns that don't generalize well to new data.\n",
    "\n",
    "2. **Avoidance of Deterioration:** Early stopping is effective in preventing overfitting because it halts training at the point where the model's performance on the validation set starts to deteriorate. This ensures that the model is selected based on its ability to generalize to new data rather than memorizing the training set.\n",
    "\n",
    "3. **Implicit Regularization:** Early stopping can be viewed as a form of implicit regularization. By stopping the training process before the model becomes overly specialized to the training data, it encourages the learning of more generalizable patterns.\n",
    "\n",
    "4. **Resource Efficiency:** Early stopping helps in saving computational resources by avoiding unnecessary training epochs. This is particularly important in deep learning, where training large models can be computationally expensive.\n",
    "\n",
    "In summary, early stopping is a practical and effective form of regularization that prevents overfitting by monitoring the model's performance on a validation set during training. It provides a balance between fitting the training data well and avoiding excessive training that could lead to poor generalization. This technique is widely used in practice to improve the efficiency and generalization performance of machine learning models, including deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a9e50-20aa-4d50-87d6-3bcb746f7d72",
   "metadata": {},
   "source": [
    "## k Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfittingH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead5ca2-f01b-417c-af01-eb4595cce1e6",
   "metadata": {},
   "source": [
    "**Batch Normalization (BatchNorm):**\n",
    "\n",
    "Batch Normalization is a technique used in deep learning to improve the training stability and speed up convergence. It normalizes the inputs of each layer in a mini-batch, making the optimization process more robust and facilitating the training of deep neural networks. BatchNorm was introduced by Sergey Ioffe and Christian Szegedy in their paper \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\"\n",
    "\n",
    "Here's a brief overview of how Batch Normalization works:\n",
    "\n",
    "1. **Normalization:** For each mini-batch during training, BatchNorm normalizes the inputs of a layer by subtracting the mean and dividing by the standard deviation of the batch.\n",
    "\n",
    "2. **Scaling and Shifting:** After normalization, the normalized values are scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta). This introduces flexibility to the normalization process and allows the model to adapt the normalized values to better suit the task.\n",
    "\n",
    "3. **Learnable Parameters:** Gamma and beta are learned during training through backpropagation, enabling the model to determine the optimal scaling and shifting for each batch.\n",
    "\n",
    "4. **Applied at Each Layer:** BatchNorm is typically applied to the inputs of each layer in a neural network, providing normalization at intermediate stages of the network.\n",
    "\n",
    "**Role of Batch Normalization as Regularization:**\n",
    "\n",
    "While the primary goal of BatchNorm is not regularization, it has been observed that BatchNorm has a regularizing effect that can contribute to preventing overfitting. Here's how BatchNorm helps in this context:\n",
    "\n",
    "1. **Reduction of Internal Covariate Shift:** BatchNorm helps in reducing internal covariate shift, which refers to the change in the distribution of activations within a network as training progresses. By normalizing the inputs at each layer, BatchNorm ensures a more stable distribution of activations, making it easier for the model to learn and preventing the model from relying too much on specific features.\n",
    "\n",
    "2. **Smoothing Effect:** BatchNorm introduces a smoothing effect during training, which can be viewed as a form of regularization. It helps to generalize better to new, unseen data by reducing sensitivity to small changes in the input distribution.\n",
    "\n",
    "3. **Effect on Learning Rate:** BatchNorm can enable the use of higher learning rates during training. This allows for faster convergence and, in some cases, better generalization.\n",
    "\n",
    "4. **Reduced Dependence on Weight Initialization:** BatchNorm makes the network less sensitive to the choice of weight initialization, which can be particularly useful in deep networks. This can contribute to improved performance on new data.\n",
    "\n",
    "While BatchNorm provides some regularizing effects, it's essential to note that it may not be sufficient as the sole regularization technique in certain cases. Often, it is used in combination with other regularization methods such as dropout or weight regularization to enhance overall model robustness and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e626c-26f9-4468-a97b-40bbf283da6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PART - 3 : APPLYING REGULARIZATON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d7ab4-40e9-41be-9d06-efb99a0c34d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ák Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropoutk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6051f478-1ad5-4072-a5e8-c33bac9ecca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler , LabelEncoder\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "datasets = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595d7f54-ec70-4eab-91ae-7a34b5e41fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(datasets.data , columns=datasets.feature_names)\n",
    "df['target'] = datasets['target']\n",
    "\n",
    "X = df.drop('target' , axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ff53238-fdbc-4978-b680-4da94c0ace3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b855ad5-c780-4daa-8c43-c296271c103d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_tem, y_train, y_tem = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tem , y_tem , test_size=0.5 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89cb8f9e-d23a-436f-876b-05ffc4e4267b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93417430-047d-46e8-ab58-eb785f050d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ann_without_drop_out():\n",
    "    \n",
    "    LAYERS = [\n",
    "        tf.keras.layers.Dense(units=30 , input_dim=30 , activation='relu' , name='InputLayer'),\n",
    "        tf.keras.layers.Dense(units=300 , activation='relu' , name='HiddenLayer1'),\n",
    "        tf.keras.layers.Dense(units=100 , activation='relu' , name='HidddenLayer2'),\n",
    "        tf.keras.layers.Dense(units=1 , activation='sigmoid' , name='OutputLayer')]\n",
    "    model = tf.keras.models.Sequential(LAYERS)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_ann_with_dropout():\n",
    "    \n",
    "    LAYERS = [\n",
    "        tf.keras.layers.Dense(units=30 , input_dim=30 , activation='relu' , name='InputLayer'),\n",
    "        tf.keras.layers.Dense(units=300 , activation='relu' , name='HiddenLayer1'),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Dense(units=100 , activation='relu' , name='HidddenLayer2'),\n",
    "        tf.keras.layers.Dense(units=1 , activation='sigmoid' , name='OutputLayer')]\n",
    "    model = tf.keras.models.Sequential(LAYERS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80d63d43-753f-4655-800a-f7083042bbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_without_do = create_ann_without_drop_out()\n",
    "\n",
    "model_with_do = create_ann_with_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d1a43-899a-4e84-bea9-6efd431bb79d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c2a43d5-aeb6-4cad-9473-e48d4ab5582b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_without_do.compile(optimizer='adam',\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "model_with_do.compile(optimizer='adam',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1706fa6-7101-4c1f-8d0f-9c47c2b15aba",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81886411-bf3c-4877-8243-47e5e3b71741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 - 2s - loss: 0.6646 - accuracy: 0.7161 - val_loss: 0.6214 - val_accuracy: 0.7412 - 2s/epoch - 116ms/step\n",
      "Epoch 2/10\n",
      "13/13 - 0s - loss: 0.5591 - accuracy: 0.7990 - val_loss: 0.4609 - val_accuracy: 0.8588 - 69ms/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "13/13 - 0s - loss: 0.3913 - accuracy: 0.8869 - val_loss: 0.2757 - val_accuracy: 0.8941 - 74ms/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "13/13 - 0s - loss: 0.2540 - accuracy: 0.8970 - val_loss: 0.1893 - val_accuracy: 0.9176 - 76ms/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "13/13 - 0s - loss: 0.1949 - accuracy: 0.9221 - val_loss: 0.1510 - val_accuracy: 0.9294 - 76ms/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "13/13 - 0s - loss: 0.1616 - accuracy: 0.9347 - val_loss: 0.1315 - val_accuracy: 0.9412 - 73ms/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "13/13 - 0s - loss: 0.1291 - accuracy: 0.9397 - val_loss: 0.1035 - val_accuracy: 0.9529 - 76ms/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "13/13 - 0s - loss: 0.1218 - accuracy: 0.9573 - val_loss: 0.0864 - val_accuracy: 0.9647 - 70ms/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "13/13 - 0s - loss: 0.1139 - accuracy: 0.9623 - val_loss: 0.1049 - val_accuracy: 0.9647 - 73ms/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "13/13 - 0s - loss: 0.1001 - accuracy: 0.9623 - val_loss: 0.0706 - val_accuracy: 0.9647 - 70ms/epoch - 5ms/step\n",
      "Epoch 1/10\n",
      "13/13 - 1s - loss: 0.6673 - accuracy: 0.6759 - val_loss: 0.6006 - val_accuracy: 0.9294 - 1s/epoch - 87ms/step\n",
      "Epoch 2/10\n",
      "13/13 - 0s - loss: 0.5833 - accuracy: 0.8367 - val_loss: 0.4501 - val_accuracy: 0.9176 - 77ms/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "13/13 - 0s - loss: 0.4405 - accuracy: 0.8618 - val_loss: 0.3105 - val_accuracy: 0.8824 - 75ms/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "13/13 - 0s - loss: 0.3333 - accuracy: 0.8869 - val_loss: 0.2106 - val_accuracy: 0.9059 - 75ms/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "13/13 - 0s - loss: 0.2538 - accuracy: 0.9070 - val_loss: 0.1650 - val_accuracy: 0.9412 - 72ms/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "13/13 - 0s - loss: 0.2248 - accuracy: 0.8995 - val_loss: 0.1491 - val_accuracy: 0.9294 - 82ms/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "13/13 - 0s - loss: 0.1924 - accuracy: 0.9296 - val_loss: 0.1157 - val_accuracy: 0.9647 - 70ms/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "13/13 - 0s - loss: 0.1656 - accuracy: 0.9397 - val_loss: 0.1005 - val_accuracy: 0.9765 - 72ms/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "13/13 - 0s - loss: 0.1543 - accuracy: 0.9422 - val_loss: 0.0954 - val_accuracy: 0.9647 - 79ms/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "13/13 - 0s - loss: 0.1402 - accuracy: 0.9573 - val_loss: 0.0803 - val_accuracy: 0.9765 - 71ms/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "history_without_do = model_without_do.fit(X_train , y_train , epochs=num_epochs , batch_size=32 , validation_data=(X_val,y_val) , verbose=2)\n",
    "\n",
    "history_with_do = model_with_do.fit(X_train,y_train,epochs=num_epochs,batch_size=32,validation_data=(X_val,y_val),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75c6ca8c-7843-41e0-851f-413c06e73f26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9884\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1114 - accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "test_loss_without_bo , test_acc_without_bo = model_without_do.evaluate(X_test , y_test)\n",
    "\n",
    "test_loss_with_bo , test_acc_with_bo = model_with_do.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "528b9e13-25ed-4538-8d88-ffb5025e0cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Without Dropout:\n",
      "Test Accuracy: 0.9884\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Without Dropout:\")\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_acc_without_bo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f0e4074-5fa5-4b9c-905e-e89bbdb9b49c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model With Dropout:\n",
      "Test Accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "print(\"Model With Dropout:\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc_with_bo:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3130b4-9d34-4050-a171-e82c088e1322",
   "metadata": {},
   "source": [
    "## ́k Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f22ba-379e-4271-88d9-658a6ad4ff74",
   "metadata": {},
   "source": [
    "Choosing the appropriate regularization technique for a deep learning task involves careful consideration of various factors and tradeoffs. Here are some key considerations to keep in mind:\n",
    "\n",
    "1. **Type of Regularization:**\n",
    "   - **L1 vs. L2 vs. Elastic Net:** Consider the characteristics of your data and model. L1 regularization promotes sparsity, which can be beneficial for feature selection. L2 regularization helps prevent large weights and encourages a more even distribution of weights. Elastic Net combines both L1 and L2 penalties.\n",
    "\n",
    "   - **Dropout:** Dropout is effective for preventing overfitting by introducing stochasticity during training. It's commonly used in conjunction with weight regularization techniques.\n",
    "\n",
    "2. **Model Architecture:**\n",
    "   - **Simple vs. Complex Models:** The level of regularization needed may depend on the complexity of your model. Simple models might require less regularization, while complex models may benefit from a combination of regularization techniques.\n",
    "\n",
    "   - **Layer-Specific Regularization:** Consider applying regularization selectively to certain layers. For example, you might apply dropout to dense layers but not to convolutional layers, or use different regularization strengths for different layers.\n",
    "\n",
    "3. **Dataset Characteristics:**\n",
    "   - **Data Size:** Regularization becomes more crucial when working with smaller datasets. In such cases, models are more prone to overfitting, and regularization helps prevent memorization of noise.\n",
    "\n",
    "   - **Feature Dimensionality:** High-dimensional datasets may benefit from techniques like L1 regularization that encourage sparsity and feature selection.\n",
    "\n",
    "4. **Training Dynamics:**\n",
    "   - **Convergence Speed:** Regularization can affect the convergence speed during training. Some regularization techniques, like dropout, may require more epochs for convergence.\n",
    "\n",
    "   - **Batch Size:** Batch normalization is an effective technique, but its performance can depend on the choice of batch size. Smaller batch sizes may have a regularizing effect due to the noise introduced during normalization.\n",
    "\n",
    "5. **Interpretability:**\n",
    "   - **Model Interpretability:** If model interpretability is crucial, L1 regularization may be preferred as it tends to drive some weights to exactly zero, resulting in a sparse model that is easier to interpret.\n",
    "\n",
    "   - **Feature Importance:** Consider whether understanding the importance of individual features is important for your application. Techniques like L1 regularization and dropout can impact feature importance differently.\n",
    "\n",
    "6. **Computational Resources:**\n",
    "   - **Computational Cost:** Some regularization techniques, like dropout, can increase computational cost during training due to the stochastic nature of dropout. Ensure that your hardware and computational resources can handle the chosen regularization method.\n",
    "\n",
    "7. **Cross-Validation:**\n",
    "   - **Hyperparameter Tuning:** Regularization strength (e.g., lambda in L1/L2 regularization, dropout rate) is a hyperparameter that should be tuned. Use techniques like cross-validation to find the optimal value for regularization strength.\n",
    "\n",
    "8. **Task Specifics:**\n",
    "   - **Nature of the Task:** The type of task (e.g., classification, regression) and the specifics of the data can influence the choice of regularization. Consider the characteristics of the data and the potential challenges specific to the task.\n",
    "\n",
    "9. **Empirical Testing:**\n",
    "   - **Experimentation:** It's often valuable to experiment with different regularization techniques and combinations to see which ones work best for your specific task. Empirical testing helps in understanding how different techniques impact model performance.\n",
    "\n",
    "10. **Ensemble Techniques:**\n",
    "   - **Ensemble Learning:** Combining multiple regularization techniques or training multiple models with different regularization settings can often result in improved generalization performance.\n",
    "\n",
    "In summary, choosing the appropriate regularization technique involves a thoughtful analysis of the specific characteristics of the task, dataset, and model architecture. It often requires empirical testing and experimentation to find the right balance between preventing overfitting and allowing the model to learn relevant patterns in the data. Regularization is not a one-size-fits-all solution, and the optimal approach may vary across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afda81-f9c4-4556-97a0-39312d7d1a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
